[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Análise e Modelagem Classificatória de Dados de Câncer de Mama",
    "section": "",
    "text": "O câncer de mama é uma doença complexa e de grande impacto na saúde, afetando milhões de mulheres e alguns homens em todo o mundo. A análise estatística e a modelagem desempenham um papel fundamental no estudo dessa doença, permitindo compreender seus fatores de risco, padrões de ocorrência e desenvolver estratégias eficazes de prevenção e tratamento personalizadas. Neste trabalho, exploraremos a importância da análise estatística na pesquisa sobre câncer de mama, abordando diferentes aspectos desse processo, como a análise de dados epidemiológicos e genéticos, a modelagem para previsão de riscos e o uso de técnicas avançadas para identificar subtipos moleculares."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Câncer de Mama: uma rápida contextualização.",
    "section": "",
    "text": "O câncer de mama é uma doença maligna que se origina no tecido mamário. É o tipo de câncer mais comum entre as mulheres em todo o mundo e também pode afetar os homens, embora seja menos frequente. Caracteriza-se pelo crescimento anormal e descontrolado das células mamárias, formando um tumor que pode se espalhar para outras partes do corpo.\n\nExistem diversos fatores de risco associados ao câncer de mama, como idade avançada, histórico familiar da doença, mutações genéticas, exposição a hormônios, obesidade, consumo excessivo de álcool, entre outros. A detecção precoce é fundamental para o sucesso do tratamento, pois permite o diagnóstico em estágios iniciais, quando as chances de cura são maiores.\nNo momento do diagnóstico, as células do câncer de mama podem ser encontradas apenas no tecido mamário, nos gânglios linfáticos axilares sob o braço ou em locais mais distantes do corpo. Com base em onde o câncer é encontrado, o câncer de mama é classificado em um estágio, de I a IV. O câncer de mama em estágio IV, também chamado de câncer de mama metastático, é o câncer de mama que se espalhou para um local do corpo distante da mama e dos gânglios linfáticos axilares."
  },
  {
    "objectID": "intro.html#dados-descritivos",
    "href": "intro.html#dados-descritivos",
    "title": "1  Câncer de Mama: uma rápida contextualização.",
    "section": "1.2 Dados Descritivos",
    "text": "1.2 Dados Descritivos\nO câncer de mama é atualmente o câncer mais comum em todo o mundo, representando \\(12,5\\%\\) de todos os novos casos anuais de câncer em todo o mundo. Aqui estão as estimativas da American Cancer Society para câncer de mama nos Estados Unidos para 2022:\n\nCerca de 13% (cerca de 1 em 8) das mulheres americanas desenvolverão câncer de mama invasivo ao longo de suas vidas.\nEm 2022, estima-se que 287.850 novos casos de câncer de mama invasivo sejam diagnosticados em mulheres nos EUA, juntamente com 51.400 novos casos de câncer de mama não invasivo (in situ).\nEspera-se que cerca de 43.250 mulheres nos EUA morram em 2022 de câncer de mama. As taxas de mortalidade por câncer de mama têm diminuído constantemente desde 1989, para um declínio geral de 43% até 2020. Acredita-se que essas reduções sejam o resultado de avanços no tratamento e detecção precoce por meio de triagem. No entanto, o declínio diminuiu ligeiramente nos últimos anos.\nO risco de uma mulher ter câncer de mama quase dobra se ela tiver um parente de primeiro grau (mãe, irmã, filha) diagnosticado com câncer de mama. Aproximadamente 15% das mulheres que têm câncer de mama têm um membro da família diagnosticado com ele.\nO câncer de mama é uma das principais causas de morte relacionada ao câncer em mulheres nos Estados Unidos, perdendo apenas para o câncer de pulmão"
  },
  {
    "objectID": "exploratory.html",
    "href": "exploratory.html",
    "title": "2  Análise exploratória de Dados",
    "section": "",
    "text": "O seguinte capítulo tem como objetivo realizar uma análise exploratória nos dados. Para isso iremos seguir o seguinte esquema:"
  },
  {
    "objectID": "exploratory.html#carregamento-dos-dados",
    "href": "exploratory.html#carregamento-dos-dados",
    "title": "2  Análise exploratória de Dados",
    "section": "2.1 Carregamento dos dados",
    "text": "2.1 Carregamento dos dados\nOs dados utilizados nesse trabalho foram retirados do Kaggle e se tratam de valores estimados a partir de uma imagem digitalizada de um aspirado com agulha fina (PAAF) de uma massa mamária com tumor. Assim, os dados descrevem características dos núcleos celulares presentes nas imagens.\nCarregamento dos dados:\n\n\nRows: 568\nColumns: 33\n$ id                      <dbl> 842302, 842517, 84300903, 84348301, 84358402, …\n$ diagnosis               <fct> M, M, M, M, M, M, M, M, M, M, M, M, M, M, M, M…\n$ radius_mean             <dbl> 17.990, 20.570, 19.690, 11.420, 20.290, 12.450…\n$ texture_mean            <dbl> 10.38, 17.77, 21.25, 20.38, 14.34, 15.70, 19.9…\n$ perimeter_mean          <dbl> 122.80, 132.90, 130.00, 77.58, 135.10, 82.57, …\n$ area_mean               <dbl> 1001.0, 1326.0, 1203.0, 386.1, 1297.0, 477.1, …\n$ smoothness_mean         <dbl> 0.11840, 0.08474, 0.10960, 0.14250, 0.10030, 0…\n$ compactness_mean        <dbl> 0.27760, 0.07864, 0.15990, 0.28390, 0.13280, 0…\n$ concavity_mean          <dbl> 0.30010, 0.08690, 0.19740, 0.24140, 0.19800, 0…\n$ concave_points_mean     <dbl> 0.14710, 0.07017, 0.12790, 0.10520, 0.10430, 0…\n$ symmetry_mean           <dbl> 0.2419, 0.1812, 0.2069, 0.2597, 0.1809, 0.2087…\n$ fractal_dimension_mean  <dbl> 0.07871, 0.05667, 0.05999, 0.09744, 0.05883, 0…\n$ radius_se               <dbl> 1.0950, 0.5435, 0.7456, 0.4956, 0.7572, 0.3345…\n$ texture_se              <dbl> 0.9053, 0.7339, 0.7869, 1.1560, 0.7813, 0.8902…\n$ perimeter_se            <dbl> 8.589, 3.398, 4.585, 3.445, 5.438, 2.217, 3.18…\n$ area_se                 <dbl> 153.40, 74.08, 94.03, 27.23, 94.44, 27.19, 53.…\n$ smoothness_se           <dbl> 0.006399, 0.005225, 0.006150, 0.009110, 0.0114…\n$ compactness_se          <dbl> 0.049040, 0.013080, 0.040060, 0.074580, 0.0246…\n$ concavity_se            <dbl> 0.05373, 0.01860, 0.03832, 0.05661, 0.05688, 0…\n$ concave_points_se       <dbl> 0.015870, 0.013400, 0.020580, 0.018670, 0.0188…\n$ symmetry_se             <dbl> 0.03003, 0.01389, 0.02250, 0.05963, 0.01756, 0…\n$ fractal_dimension_se    <dbl> 0.006193, 0.003532, 0.004571, 0.009208, 0.0051…\n$ radius_worst            <dbl> 25.38, 24.99, 23.57, 14.91, 22.54, 15.47, 22.8…\n$ texture_worst           <dbl> 17.33, 23.41, 25.53, 26.50, 16.67, 23.75, 27.6…\n$ perimeter_worst         <dbl> 184.60, 158.80, 152.50, 98.87, 152.20, 103.40,…\n$ area_worst              <dbl> 2019.0, 1956.0, 1709.0, 567.7, 1575.0, 741.6, …\n$ smoothness_worst        <dbl> 0.1622, 0.1238, 0.1444, 0.2098, 0.1374, 0.1791…\n$ compactness_worst       <dbl> 0.6656, 0.1866, 0.4245, 0.8663, 0.2050, 0.5249…\n$ concavity_worst         <dbl> 0.71190, 0.24160, 0.45040, 0.68690, 0.40000, 0…\n$ concave_points_worst    <dbl> 0.26540, 0.18600, 0.24300, 0.25750, 0.16250, 0…\n$ symmetry_worst          <dbl> 0.4601, 0.2750, 0.3613, 0.6638, 0.2364, 0.3985…\n$ fractal_dimension_worst <dbl> 0.11890, 0.08902, 0.08758, 0.17300, 0.07678, 0…\n$ x33                     <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nNota-se que o conjunto de dados possui 568 observações com 33 variaveis\n\ndata_cancer |> visdat::vis_miss()\n\n\n\n\nA partir do gráfico de valores faltantes, nota-se que apenas uma coluna possui NAs, assim ela foi removida do conjunto de dados (variavel não apresentava nenhum valor e provalvelmente é resquício de um erro no upload dos dados por parte UCI Machine Learning Repositor)\n\ndata_cancer = data_cancer|>\n  dplyr::select(-'x33' )\n\n\ndata_cancer |> head()\n\n# A tibble: 6 × 32\n        id diagnosis radius_mean texture_mean perimeter_mean area_mean\n     <dbl> <fct>           <dbl>        <dbl>          <dbl>     <dbl>\n1   842302 M                18.0         10.4          123.      1001 \n2   842517 M                20.6         17.8          133.      1326 \n3 84300903 M                19.7         21.2          130       1203 \n4 84348301 M                11.4         20.4           77.6      386.\n5 84358402 M                20.3         14.3          135.      1297 \n6   843786 M                12.4         15.7           82.6      477.\n# ℹ 26 more variables: smoothness_mean <dbl>, compactness_mean <dbl>,\n#   concavity_mean <dbl>, concave_points_mean <dbl>, symmetry_mean <dbl>,\n#   fractal_dimension_mean <dbl>, radius_se <dbl>, texture_se <dbl>,\n#   perimeter_se <dbl>, area_se <dbl>, smoothness_se <dbl>,\n#   compactness_se <dbl>, concavity_se <dbl>, concave_points_se <dbl>,\n#   symmetry_se <dbl>, fractal_dimension_se <dbl>, radius_worst <dbl>,\n#   texture_worst <dbl>, perimeter_worst <dbl>, area_worst <dbl>, …\n\n\nAs 5 primeiras linhas do conjunto de dados mostram que das 32 variáveis remanescentes, 30 são numericas, 1 categórica e a restante representa um ID do paciente analisado. Os modelos desenvolvidos nesse trabalho buscam classificar o diagnóstico do paciente (tumor maligno ou benigno) com base nas variáveis numéricas. Assim é importantíssima a análise de tais variáveis, estudando valores de correlação, aderência e significância\nUm estudo de simulação, assim como a documentação dos dados indica que não há falta de informação ao se trabalhar apenas com as variáveis terminadas com mean como explicativas, assim, trabalhou-se apenas como aquelas terminadas em mean como variávies explicativas para a variável resposta diagnosis:\n\ndata_cancer = data_cancer |>\n  dplyr::select(id, diagnosis , dplyr::contains('_mean'))\n\nO conjunto de dados possui 568 observações com 12 colunas restantes após a retirada das demais. Das 12, uma é o ID de cada observação e uma é a variável de interesse diagnosis.\nAs variáveis restantes são:\n\nradius_mean: Média das distâncias do centro aos pontos do perímetro\ntexture_mean: Média das diferenças dos valores da escala de cinza (visualização celular via microscópío eletrónico)\nperimeter_mean: Tamanho médio do tumor central\narea_mean: Área média do tumor central\nsmoothness_mean: Média de variação local em comprimentos de raio\ncompactness_mean: Função direta do perímetro e da área: \\(=\\frac{perímetro^2}{area-1}\\)\nconcavity_mean: Média da severidade das porções côncavas do contorno\nconcavity_points_mean: Média para o número de porções côncavas do contorno\nsymmetry_mean:\nfractal_dimension_mean:\n\nPara o estudo, há a necessidade de verificação do balanceamento do conjunto de dados em relação ao diagnóstico do paciente (tumor maligno ou benigno).\nTal verificação foi feita em cima de análise gráfica, e via Coeficiente de Entropia de Shannon\n\n\n\n\nprop_df = data_cancer |>\n  dplyr::reframe(prop_B = mean(diagnosis == 'B'), prop_M = mean(diagnosis == 'M')) |>\n  tidyr::pivot_longer(c(prop_B, prop_M))\n\ndata_cancer |> \n  ggplot2::ggplot(ggplot2::aes(x = diagnosis, fill = diagnosis)) +\n  ggplot2::geom_bar() +\n  ggplot2::theme_minimal() +\n  ggthemes::scale_fill_colorblind() +\n  ggtitle(\"Contagem de Diagnósticos por Grupo\")\n\n\n\n\nAlem da visualização gráfica, foi utilizado o Coeficiente de Entropia de Shannon, que vai de 0 a 1, onde 0 indica dados totalmente desbalanceados (proporção \\(0\\%\\) e \\(100\\%\\)) e 1 balanceamento completo (proporção \\(50\\%\\) e \\(50\\%\\))\n\nDescTools::Entropy(data_cancer$diagnosis |> table())\n\n[1] 0.953127\n\n\nO Coeficiente calculado foi de: \\(0.953127\\)\nAssim, a partir do grafico de barras, proporções de cada grupo e teste de Entropia de Shannon, não há evidencias para um desbalanceamento significativo no conjunto de dados utilizados.\nO próximo tópico busca realizar uma análise exploratória nos dados. Antes porém, buscando diminuir o viés amostral, o conjunto de dados foi divido em treino e teste, com proporção \\(80\\% \\ e \\ 20\\%\\) (Arafa et al. 2021), o metodo de divisão utilizado foi de amostragem estratificada simples, tal método foi escolhido para evitar desbalanceamento de diagnosticos malignos e benignos nos conjuntos de treino e teste. Portanto, toda a análise e modelagem realizada nos próximos tópicos foi em cima do conjunto de dados de teste.\n\nset.seed(607) #Fixando semente para reprodução dos resultados \ndata_cancer_split = rsample::initial_split(data_cancer, prop = 0.80, strata = diagnosis)\ndata_cancer_train = rsample::training(data_cancer_split)\ndata_cancer_test = rsample::testing(data_cancer_split)"
  },
  {
    "objectID": "exploratory.html#análise-e-interpretação-das-variaveis",
    "href": "exploratory.html#análise-e-interpretação-das-variaveis",
    "title": "2  Análise exploratória de Dados",
    "section": "2.2 Análise e interpretação das variaveis",
    "text": "2.2 Análise e interpretação das variaveis\nBuscando observar o comportamento de cada variável dividida por grupo de tratamento, construi-se o seguinte gráfico:\n\nlibrary(tidyverse) \n\ndata_cancer_train |> \n  dplyr::select(-id) |>\n  pivot_longer(-diagnosis) |>\n  ggplot(aes(x = value, fill = diagnosis)) +\n  geom_histogram(aes(y = after_stat(density))) +\n  facet_wrap(~name, scales = \"free\") +\n  ggthemes::scale_fill_colorblind()\n\n\n\n\nObservando os gráficos vemos que a variável area celular média divide bem os 2 grupos, assim como concavidade. Simetria e textura foram variáveis que não dividiram tão bem os grupos, onde eles se apresentam sobrepostos\nDada o número de variáveis do conjunto de dados, realizar uma análise individual seria demorada e tediosa, podendo mostrar pouco avanço na interpretação dos dados. Assim utilizou-se a técnica de redução de dimensionalidade para um melhor entendimento das relações entre as variáveis"
  },
  {
    "objectID": "exploratory.html#redução-de-dimensionalidade",
    "href": "exploratory.html#redução-de-dimensionalidade",
    "title": "2  Análise exploratória de Dados",
    "section": "2.3 Redução de dimensionalidade",
    "text": "2.3 Redução de dimensionalidade\nDados do mundo real, como sinais de fala, fotografias digitais ou varreduras de fMRI, geralmente têm alta dimensionalidade. Para lidar adequadamente com os dados do mundo real, dimensionalidade precisa ser reduzida. Redução de dimensionalidade é a transformação de dados de alta dimensão em uma representação significativa de dimensionalidade reduzida. Idealmente, a representação reduzida deve ter uma dimensionalidade que corresponde à dimensionalidade intrínseca dos dados (Van Der Maaten et al. 2009)\nDado a alta dimensionalidade dos dados trabalhados e buscando observar relações entre as variáveis analisadas, utilizou-se os métodos de Ánalise de Componentes Principais e Análise Fatorial para isso\n\n2.3.1 Análise de Componentes Principais\nAnálise de Componentes Principais (PCA) constroi uma representação de baixa dimensão dos dados que descreve o máximo possível da variância dos dados.\nA técnica se baseia no cálculo de autovalores e autovetores, buscando assim associção lineares entre as variáveis.\n\npr_cancer = data_cancer_train |>\n  dplyr::select(dplyr::contains('_mean')) |> \n  princomp() \n\n\npr_cancer |> factoextra::fviz_eig()\n\n\n\n\nObservando o gráfico da variabilidade captada por cada componente, viu-se que a primeira dimensão captou grande parte dessa variabilidade e assim, trabalhou-se apenas com uma dimensão na análise dos dados via PCA\nBuscando visualizar quanto cada variável contribui na primeira componente construiu-se o seguinte gráfico\n\npr_cancer |> factoextra::fviz_contrib(choice = \"var\", axes = c(1))\n\n\n\n\nViu-se que a variável area_mean foi predominante na primeira componente. As demais variáveis não foram captadas significativamente pela primeira componente, porém, ao se rankear elas, observou-se uma maior importância daquelas variáveis correspondentes ao questões de tamanho celulares, como: area-mean, perimeter_mean, radius_mean\nAlém disso, tais variáveis são altamente linearmente dependentes, onde tanto área como perímetro são função do raio.\nOnde tem-se os seguintes valores de correlação:\n\ndata_cancer_train |> \n  dplyr::select(area_mean, perimeter_mean, radius_mean) |> \n  cor() |>\n  ggcorrplot::ggcorrplot(hc.order = TRUE, \n                         type = \"lower\",\n                         lab = TRUE,\n                         colors = c(\"#6D9EC1\", \"white\", \"#E46726\"))\n\n\n\n\nAssim, buscando eliminar multicolinearidade dos dados, eliminou-se as variáveis: perimeter_mean, radius_mean. A eliminação dessa variáveis não acarreta e perda de informações dado que área é função de raio e perímetro\nRealizando novamente a redução de dimensionalidade, mas agora sem as variáveis perimeter_mean, radius_mean, tem-se:\n\npr_cancer = data_cancer_train |>\n  dplyr::select(contains(\"_mean\"),-perimeter_mean, -radius_mean) |>\n  princomp() \n\npr_cancer |> factoextra::fviz_eig()\n\n\n\n\nObservando o gráfico, viu-se que novamente a primeira componente captou cerca de \\(100\\%\\) da variabilidade dos dados\n\npr_cancer |> factoextra::fviz_contrib(choice = \"var\", axes = c(1))\n\n\n\n\nE novamente a variável area_mean foi aquele de maior importância na primeira componente\nPlotando os valores de escore por grupo, construi-se o seguinte gráfico:\n\nfactoextra::fviz_pca_ind(pr_cancer,\n             label = \"none\", # hide individual labels\n             habillage = data_cancer_train$diagnosis, # color by groups\n             palette = c('black', 'yellow'),\n             addEllipses = TRUE # Concentration ellipses\n             )\n\n\n\n\nÉ possivel notar que o grupo de tumor benigno se possui uma variabilidade menor que o grupo de tumor maligno. Além disso, o grupo de tumor benigno se concentra em valores negativos e perto de 0 no eixo X, enquanto o grupo de tumor maligno se concentra em valores positivos no eixo X.\nSegundo Karhunen and Joutsensalo (1995), quando a primeira componente captura cerca de \\(100\\%\\) da variabilidade dos dados, isso pode indicar que todas as variáveis podem ser escritas como uma transformação linear de uma delas, isso é, em algebra linear, todas as variáveis são linearmente dependentes e dado que cada componente é uma base vetorial, necessita-se apenas de uma base para escrever todas os vetores desse sistema\nAssim, buscando contornar tal problema, foi utilizado o método de engenharia de características.\nEngenharia de características (feature engineering) é o processo de selecionar, transformar e criar atributos (características) relevantes a partir dos dados brutos para melhorar o desempenho e eficácia de modelos (Nargesian et al. 2017). O grupamento de características pode ser feito de 2 formas:\n\nUtilizando conhecimento prévio na área de estudo, e portanto agrupando variáveisem um que agrupamento faça sentido\nUtilizando redução de dimensionalidade, é possível usar técnicas como PCA ou FA para reduzir a complexidade dos dados, identificando combinações lineares de atributos que expliquem a maior parte da variabilidade.\n\nAlém disso, é possivel combinar as duas abordagens visando melhores transformações.\nUtilizou-se a abordagem de redução de dimensionalidade via FA para o agrupamento de características\n\n\n2.3.2 Análise Fatorial\nAnálise fatorial é uma técnica estatística que busca identificar padrões ou estruturas subjacentes em um conjunto de variáveis observadas, reduzindo sua dimensionalidade e representando-as em termos de fatores latentes que explicam sua covariância.\nO modelo fatorial pode utilizar diversos métodos de factorização, as mais conhecidas e aquelas que foram utilizadas no trabalho são:\n\nResíduos Mínimos\nMáxima Verossimilhança\nFatores Principais\n\nAlém disso, foi aplicado a rotação varimax nos 3 modelos, onde tal método auxiliou na interpretação de varivaies dentre de cada fator\n\nfa_rm_cancer = data_cancer_train |>\n                dplyr::select(dplyr::contains('_mean'), -'perimeter_mean', -'radius_mean') |>\n  psych::fa(nfactors = 3, rotate = 'varimax', fm = 'minres') \n  \n  \nfa_ml_cancer = data_cancer_train |>\n                dplyr::select(dplyr::contains('_mean'), -'perimeter_mean', -'radius_mean') |>\n  psych::fa(nfactors = 3, rotate = 'varimax', fm = 'ml') \n\nfa_pcomp_cancer = data_cancer_train |>\n                dplyr::select(dplyr::contains('_mean'), -'perimeter_mean', -'radius_mean') |>\n  psych::fa(nfactors = 3, rotate = 'varimax', fm = 'pa') \n\nApós o calculo dos fatores para cada modelo, construi-se os seguintes gráficos\n\nfa_rm_cancer |> psych::fa.diagram(main = 'Modelo via Resíduos Minimos')\n\n\n\nfa_ml_cancer |> psych::fa.diagram(main = 'Modelo via Máxima Verossimilhança')\n\n\n\nfa_pcomp_cancer |> psych::fa.diagram(main = 'Modelo via Fatores Principais')\n\n\n\n\nÉ possível ver que a utilização de 2 fatores foi suficiente em 2 dos 3 modelos construidos (Resíduos Mínimos e Fatores Principais). O modelo de via Máxima Verossimilhança utilizou um terceiro fator para a variável smoothness_mean ser separada das demais.\nAssim, observando os fatores os interpretando, tem-se que:\n\nFator 1: Esse fator pode ser interpretado como um indicador de características relacionadas à forma e ao tamanho das células. As variáveis com maiores loadings, como área celular, pontos concavos, concavidade média e textura celular, sugerem que esse fator está relacionado à irregularidade e complexidade das células, possivelmente refletindo a presença de células com formas anormais ou características específicas associadas ao câncer de mama.\nFator 2: Esse fator pode ser interpretado como um indicador de características relacionadas à estrutura e textura das células. As variáveis com maiores loadings, como dimensão fractal, suavidade celular, compacidade celular e simetria celular, sugerem que esse fator está relacionado à organização estrutural das células, possivelmente refletindo a uniformidade ou padrões na estrutura celular. Pode indicar diferenças nas propriedades estruturais das células cancerígenas em comparação com células saudáveis.\n\nA partir dessas interpretções descritas, foi possivel construir novas variáveis, as chamadas features."
  },
  {
    "objectID": "exploratory.html#engenharia-de-características",
    "href": "exploratory.html#engenharia-de-características",
    "title": "2  Análise exploratória de Dados",
    "section": "2.4 Engenharia de Características",
    "text": "2.4 Engenharia de Características\nUtilizando os modelos fatoriais construídos no tópico anterior, foram construidas duas novas variaveis, onde essas foram utilizadas no tópico de modelagem:\nTODAS AS VARIÁVEIS FORAM PADRONIZADAS ANTES DE SEREM UTILIZADAS NAS FUNÇÕES\n\nanomaly_feature = Característica de anomalia celular. É dada pela seguinte função\n\nanomaly_features: media(area_mean, concave_points_mean, concavity_mean, texture_mean)\n\nstructure_feature: Característica de estrutura celular. É dada pela seguinte função:\n\nstructure_feature = media(compactness_mean, fractal_dimension_mean, smoothness_mean, symmetry_mean)\n\nlibrary(tidymodels)\n\nvariables_to_remove = data_cancer |>\n  dplyr::select(contains(\"_mean\")) |>\n  names()\n\ncancer_recipes = recipes::recipe(diagnosis ~ .,\n                                data = data_cancer_train) |>\n      recipes::update_role(id, new_role = \"id\") |>\n      step_normalize(all_predictors()) |>\n      step_mutate(anomaly_feature = (area_mean + concave_points_mean + concavity_mean + texture_mean)/4,\n                  structure_feature = (compactness_mean + fractal_dimension_mean + smoothness_mean + symmetry_mean)/4) |>\n  update_role(all_of(variables_to_remove), new_role = \"ignore\")\n\ndata_cancer_features = cancer_recipes |> \n  prep() |> \n  bake(new_data = NULL) |>\n  dplyr::select(-contains(\"_mean\")) \n\n\n\n\n\nArafa, Ahmed, Marwa Radad, Mohammed Badawy, and Nawal El-Fishawy. 2021. “Regularized Logistic Regression Model for Cancer Classification.” In 2021 38th National Radio Science Conference (NRSC), 1:251–61. IEEE.\n\n\nKarhunen, Juha, and Jyrki Joutsensalo. 1995. “Generalizations of Principal Component Analysis, Optimization Problems, and Neural Networks.” Neural Networks 8 (4): 549–62.\n\n\nNargesian, Fatemeh, Horst Samulowitz, Udayan Khurana, Elias B Khalil, and Deepak S Turaga. 2017. “Learning Feature Engineering for Classification.” In Ijcai, 17:2529–35.\n\n\nVan Der Maaten, Laurens, Eric O Postma, H Jaap van den Herik, et al. 2009. “Dimensionality Reduction: A Comparative Review.” Journal of Machine Learning Research 10 (66-71): 13."
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "3  Modelos de Classificação",
    "section": "",
    "text": "A classificação correta de tumores benignos e malignos desempenha um papel crucial no diagnóstico precoce e tratamento eficaz do câncer. Nesse contexto, os modelos de classificação surgem como ferramentas promissoras na área médica, permitindo a análise automatizada de características clínicas, genéticas e radiológicas para diferenciar entre tumores benignos e malignos. Esses modelos utilizam algoritmos de aprendizado de máquina para extrair padrões relevantes dos dados e fazer previsões precisas. Ao fornecer informações valiosas para médicos e especialistas, os modelos de classificação podem auxiliar na identificação de pacientes em risco, direcionar tratamentos personalizados e melhorar os resultados clínicos. Neste trabalho, exploraremos diferentes abordagens de modelos de classificação e discutiremos sua aplicação na discriminação entre tumores benignos e malignos, com o objetivo de aprimorar a detecção precoce e o tratamento eficiente do câncer.\nA documentação dos dados cita que tal classificação indica o proseguindo ou não do paciente para exames mais invasivos, tal sequencia é ilustrada pelo seguinte diagrama:\nPorém, assim como todos modelos estatísticos, os modelos de classificação apresentam erros, tais erros de classificação são denominados Falso Positivo e Falso Negativo, onde no caso dos dados trabalhados:\nTal definição é importante para a definição de limites e custos de erros.\nO custo de erro associado ao erro Falso Negativo é maior que o Falso Positivo pois indica que um paciente com tumor maligno não deverá avancar em seu tratamento, podendo levar o mesmo a um estágio mortal da doença. Já a classificação Falso Positivo indica que o paciente sem tumor maligno avançe na diagnóstico da doença, onde serão utilizados metodos mais invasivos para isso\nPortanto, o diagrama real é dada por:"
  },
  {
    "objectID": "models.html#análise-de-discriminante",
    "href": "models.html#análise-de-discriminante",
    "title": "3  Modelos de Classificação",
    "section": "3.1 Análise de Discriminante",
    "text": "3.1 Análise de Discriminante\nO objetivo da análise discriminante é encontrar uma função discriminante que maximize a separação entre os grupos, levando em consideração a estrutura das variáveis preditoras.\nDado o contexto do problema estudado, a função discriminante estimada foi aquela que maximizou a separação entre os grupos de pessoas com tumor benigno e maligno.\n\n3.1.1 Análise via Discriminante Linear\nO discriminante linear é o mais simples dentre os modelos discriminantes. Ele busca separar os grupos de analises via uma reta, maximizando a distancia entre os grupos\n\nlibrary(discrim)\n\ndata_cancer_train = cancer_recipe |> prep() |> bake(new_data = NULL)\ndata_cancer_test = cancer_recipe |> prep() |> bake(new_data = data_cancer_test)\n\ncross_folds = rsample::vfold_cv(data_cancer_train, strata = diagnosis)\n\nPara uma estimação robusta, utilizou-se o método de cross validation, que se trata da divisão do conjunto de treino em determinado número de folds, no caso foram dividos 10 folds. A cada iteração 9 folds eram usados como fonte de dados para otimização dos parametros do modelo e o fold restante era utilizado como validação das métricas. Ao final, obteu-se as seguintes metricas\nPara cada metrica, foi construido um intervalo de confiança de \\(95\\%\\)\n\nlda_spec = discrim_linear() |>\n  set_mode('classification') |>\n  set_engine('MASS')\n\nlda_fit = wf |>\n  workflows:: add_model(lda_spec) |>\nparsnip::fit(data = data_cancer_train)\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\n\nlda_metrics = lda_fit |> \n  tune::fit_resamples(cross_folds,\n    metrics = yardstick::metric_set(accuracy, roc_auc, sens, j_index, specificity),\n    control = tune::control_resamples(save_pred = TRUE)) |>\n  workflowsets::collect_metrics() |>\n  mutate(lower_bound = mean - 1.96*std_err, upper_bound = mean + 1.96*std_err) \n\nlda_metrics|>\n  kableExtra::kbl() %>%\n  kableExtra::kable_material(c(\"striped\", \"hover\"))\n\n\n\n \n  \n    .metric \n    .estimator \n    mean \n    n \n    std_err \n    .config \n    lower_bound \n    upper_bound \n  \n \n\n  \n    accuracy \n    binary \n    0.9029403 \n    10 \n    0.0180452 \n    Preprocessor1_Model1 \n    0.8675716 \n    0.9383089 \n  \n  \n    j_index \n    binary \n    0.7518745 \n    10 \n    0.0436509 \n    Preprocessor1_Model1 \n    0.6663186 \n    0.8374303 \n  \n  \n    roc_auc \n    binary \n    0.9712103 \n    10 \n    0.0053401 \n    Preprocessor1_Model1 \n    0.9607437 \n    0.9816770 \n  \n  \n    sens \n    binary \n    0.9823892 \n    10 \n    0.0079262 \n    Preprocessor1_Model1 \n    0.9668538 \n    0.9979246 \n  \n  \n    specificity \n    binary \n    0.7694853 \n    10 \n    0.0375090 \n    Preprocessor1_Model1 \n    0.6959676 \n    0.8430030 \n  \n\n\n\n\n\n\n\n3.1.2 Análise via Discriminante Quadrático\nO modelo de discrimante quadrático é mais utilizado quando as matrizes de covariancias são diferentes entre os grupo.\n\nqda_spec = discrim_quad() |>\n  set_mode('classification') |>\n  set_engine('MASS')\n\nqda_fit = wf |>\n  add_model(qda_spec) |>\n  fit(data = data_cancer_train)\n\nPara uma estimação robusta, utilizou-se novamente o método de cross validation\n\nqda_fitted_resamples = qda_fit |> \n  fit_resamples(cross_folds,\n    metrics = metric_set(accuracy, roc_auc, sens, j_index, specificity),\n    control = control_resamples(save_pred = TRUE))\n\nqda_metrics =  qda_fitted_resamples |>\n  collect_metrics() |>\n  mutate(lower_bound = mean - 1.96*std_err, upper_bound = mean + 1.96*std_err)\n\nqda_metrics |> \n  kableExtra::kbl() %>%\n  kableExtra::kable_material(c(\"striped\", \"hover\"))\n\n\n\n \n  \n    .metric \n    .estimator \n    mean \n    n \n    std_err \n    .config \n    lower_bound \n    upper_bound \n  \n \n\n  \n    accuracy \n    binary \n    0.9098507 \n    10 \n    0.0196930 \n    Preprocessor1_Model1 \n    0.8712523 \n    0.9484490 \n  \n  \n    j_index \n    binary \n    0.7915496 \n    10 \n    0.0437582 \n    Preprocessor1_Model1 \n    0.7057836 \n    0.8773155 \n  \n  \n    roc_auc \n    binary \n    0.9681723 \n    10 \n    0.0062264 \n    Preprocessor1_Model1 \n    0.9559685 \n    0.9803760 \n  \n  \n    sens \n    binary \n    0.9511084 \n    10 \n    0.0173165 \n    Preprocessor1_Model1 \n    0.9171680 \n    0.9850488 \n  \n  \n    specificity \n    binary \n    0.8404412 \n    10 \n    0.0339273 \n    Preprocessor1_Model1 \n    0.7739437 \n    0.9069386"
  },
  {
    "objectID": "models.html#escolha-do-melhor-discriminante",
    "href": "models.html#escolha-do-melhor-discriminante",
    "title": "3  Modelos de Classificação",
    "section": "3.2 Escolha do Melhor Discriminante",
    "text": "3.2 Escolha do Melhor Discriminante\nObservando as estimativas obtidas, viu-se que ambos os modelos apresentaram desempenho semelhantes. Assim utilizou-se como critério principal aquele que obteve o melhor desempenho na métrica sensibilidade dado que o contexto do problema trabalho indica um maior custo de erro para Falso Negativo\nPara melhor visualização, construi-se a seguinte tabela comparando o valor da sensibilidade de ambos os modelos:\n\nbind_rows(lda_metrics, qda_metrics) |>\n  filter(.metric == 'sens') |>\n  dplyr::select(-.config, -.estimator) |>\n  mutate(model = c('LDA', 'QDA')) |>\n  relocate(model) |>\n  kableExtra::kbl() %>%\n  kableExtra::kable_material(c(\"striped\", \"hover\"))\n\n\n\n \n  \n    model \n    .metric \n    mean \n    n \n    std_err \n    lower_bound \n    upper_bound \n  \n \n\n  \n    LDA \n    sens \n    0.9823892 \n    10 \n    0.0079262 \n    0.9668538 \n    0.9979246 \n  \n  \n    QDA \n    sens \n    0.9511084 \n    10 \n    0.0173165 \n    0.9171680 \n    0.9850488 \n  \n\n\n\n\n\nNovamente, ambos os modelos se mostraram extramamentes semelhantes, porém notou-se um ligeiro melhor desempenho por parte do modelo Linear. Assim, escolheu-se ele para a etapa de final de escolha de melhor modelo"
  },
  {
    "objectID": "models.html#regressão-logística",
    "href": "models.html#regressão-logística",
    "title": "3  Modelos de Classificação",
    "section": "4.1 Regressão Logística",
    "text": "4.1 Regressão Logística\nA regressão logística é um modelo estatístico utilizado para prever a probabilidade de ocorrência de um evento binário\nNo contexto do problema trabalhado, utilizou-se a função de ligação logito\n\nlogistic_spec = parsnip::logistic_reg() |>\n  parsnip::set_engine(\"glm\", family = stats::binomial(link='logit')) |>\n  parsnip::set_mode(\"classification\")\n\nPara uma estimação robusta, utilizou-se novamente o método de cross validation\n\nlr_fit = wf |>\n  workflows::add_model(logistic_spec) |>\n  parsnip::fit(data_cancer_train) \n\n\nlr_fitted_resamples = lr_fit |>\n  fit_resamples(cross_folds,\n    metrics = metric_set(accuracy, roc_auc, sens, j_index, specificity),\n    control = control_resamples(save_pred = TRUE))\n\nlr_metrics = lr_fitted_resamples |> \n  collect_metrics() |>\n  mutate(lower_bound = mean - 1.96*std_err, upper_bound = mean + 1.96*std_err)\n\nlr_metrics|>\n  kableExtra::kbl() %>%\n  kableExtra::kable_material(c(\"striped\", \"hover\"))\n\n\n\n \n  \n    .metric \n    .estimator \n    mean \n    n \n    std_err \n    .config \n    lower_bound \n    upper_bound \n  \n \n\n  \n    accuracy \n    binary \n    0.9161770 \n    10 \n    0.0173094 \n    Preprocessor1_Model1 \n    0.8822506 \n    0.9501034 \n  \n  \n    j_index \n    binary \n    0.8065162 \n    10 \n    0.0380323 \n    Preprocessor1_Model1 \n    0.7319730 \n    0.8810595 \n  \n  \n    roc_auc \n    binary \n    0.9726855 \n    10 \n    0.0053890 \n    Preprocessor1_Model1 \n    0.9621231 \n    0.9832479 \n  \n  \n    sens \n    binary \n    0.9543103 \n    10 \n    0.0149094 \n    Preprocessor1_Model1 \n    0.9250879 \n    0.9835328 \n  \n  \n    specificity \n    binary \n    0.8522059 \n    10 \n    0.0279981 \n    Preprocessor1_Model1 \n    0.7973296 \n    0.9070821 \n  \n\n\n\n\n\nO modelo de Regressão Logistica apresentou um desempenho semelhante ao de discriminates, uma análise mais detalaha foi feito nos próximos tópicos"
  },
  {
    "objectID": "models.html#randon-forest",
    "href": "models.html#randon-forest",
    "title": "3  Modelos de Classificação",
    "section": "4.2 Randon Forest",
    "text": "4.2 Randon Forest\nRandom Forest é um algoritmo de aprendizado de máquina baseado em árvores de decisão. Ele combina várias árvores para criar um modelo robusto e preciso. Cada árvore é treinada em uma amostra aleatória e faz previsões independentes, e o resultado final é obtido por votação ou média. É eficaz em problemas de classificação e regressão, lidando com sobreajuste e identificando características importantes.\nNo contexto do problema trabalhado, a random forest foi definida para classificação.\nAlém disso, foi utilizado o metodo de otimização de parâmetros para a construção de um melhor modelo\n\nlibrary(randomForest)\n\nrand_forest_spec = rand_forest(\n  mtry = tune(),\n  trees = 2000,\n  min_n = tune()\n) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"randomForest\")\n\n\nrand_forest_fit = wf |>\n  workflows::add_model(rand_forest_spec) \n\ndoParallel::registerDoParallel()\n\nrand_forest_tune = rand_forest_fit |>\n  tune_grid(\n    resamples = cross_folds,\n    metrics = metric_set(accuracy, roc_auc, sens, j_index, specificity),\n    grid = 10)\n\nrand_forest_tune |>\n  collect_metrics() %>%\n  filter(.metric == 'sens') %>%\n  dplyr::select(mean, min_n, mtry) %>%\n  pivot_longer(min_n:mtry,\n    values_to = \"value\",\n    names_to = \"parameter\"\n  ) %>%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"sens\")\n\n\n\nrand_forest_tune |>\n  collect_metrics() %>%\n  filter(.metric == 'roc_auc') %>%\n  dplyr::select(mean, min_n, mtry) %>%\n  pivot_longer(min_n:mtry,\n    values_to = \"value\",\n    names_to = \"parameter\"\n  ) %>%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"AUC\")\n\n\n\n\nObservando os valores dos hiperparametros min_n e mtry vemos um intervalo definido em que eles apresentaram melhor desempenho para a otimização da area sob a curve ROC e para a sensibilidade do modelo\nAssim, foi realziado um estudo maior sobre esse área de otimização\n\ndoParallel::registerDoParallel()\n grid_table = crossing(min_n = seq(10,11, by=.1),\n          mtry = c(0,1, by = 0.1))\n\nrand_forest_tune = rand_forest_fit |>\n  tune_grid(\n    resamples = cross_folds,\n    metrics = metric_set(accuracy, roc_auc, sens, j_index, specificity),\n    grid = grid_table)\n\nAssim, os parametros ótimos são:\n\nmin_n = 10.3\nmtry = 0.1\n\n\nrand_forest_spec = rand_forest(\n  mtry = 0.1,\n  trees = 2000,\n  min_n = 10.3\n) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"randomForest\")\n\nrand_forest_fit = wf |>\n  workflows::add_model(rand_forest_spec) \n\nrand_forest_fitted_resamples = rand_forest_fit |>\n  fit_resamples(\n    resamples = cross_folds,\n    metrics = metric_set(accuracy, roc_auc, sens, j_index, specificity)\n    )\n\nPara uma estimação robusta das metricas, utilizou-se novamente o método de cross validation\n\nrand_forest_fitted_resamples = rand_forest_fitted_resamples |> \n  collect_metrics() \n\nrand_forest_fitted_resamples |>\n    mutate(lower_bound = mean - 1.96*std_err, upper_bound = mean + 1.96*std_err) |>\n    kableExtra::kbl() %>%\n    kableExtra::kable_material(c(\"striped\", \"hover\"))\n\n\n\n \n  \n    .metric \n    .estimator \n    mean \n    n \n    std_err \n    .config \n    lower_bound \n    upper_bound \n  \n \n\n  \n    accuracy \n    binary \n    0.9293127 \n    10 \n    0.0146846 \n    Preprocessor1_Model1 \n    0.9005309 \n    0.9580945 \n  \n  \n    j_index \n    binary \n    0.8464322 \n    10 \n    0.0312608 \n    Preprocessor1_Model1 \n    0.7851610 \n    0.9077034 \n  \n  \n    roc_auc \n    binary \n    0.9743009 \n    10 \n    0.0050554 \n    Preprocessor1_Model1 \n    0.9643923 \n    0.9842095 \n  \n  \n    sens \n    binary \n    0.9471675 \n    10 \n    0.0141699 \n    Preprocessor1_Model1 \n    0.9193944 \n    0.9749406 \n  \n  \n    specificity \n    binary \n    0.8992647 \n    10 \n    0.0216479 \n    Preprocessor1_Model1 \n    0.8568349 \n    0.9416945 \n  \n\n\n\n\n\nO modelo de Random Forest apresentou um desempenho semelhante aos demais modelos, uma análise mais detalhada foi feito nos próximos tópicos"
  },
  {
    "objectID": "models.html#interpretação-e-métricas-do-modelo-nos-dados-de-teste",
    "href": "models.html#interpretação-e-métricas-do-modelo-nos-dados-de-teste",
    "title": "3  Modelos de Classificação",
    "section": "5.1 Interpretação e Métricas do Modelo nos Dados de Teste",
    "text": "5.1 Interpretação e Métricas do Modelo nos Dados de Teste\nNo início do tópico de exploração de dados, os dados foram dividos em treino em teste, onde toda a etapa de exploração e modelagem foi realizado em cima do conjunto de dados de treino.\nO seguinte tópico busca observar o comportamento do modelo selecionado como o melhor entre os 3 construidos no conjunto de dados de treino. A divisão de dados ajuda a reduzir a viés amostral e ajuda a observar a capacidade de generalização do modelo em novos dados.\nO modelo selecionado foi o de Regressão Logistica, que possui a seguinte forma:\n\\[ln(\\frac{Y}{1-Y})=\\beta_0 + \\beta_1*anomaly\\_feature + \\beta_2*structure\\_feature\\]\nCom os Betas estimados de:\n\\[\\beta_0 = -19.00366, e \\ ROR_{beta_0}=exp(-19.00366) = 5.5 *10^{-9}\\] \\[\\beta_1 = 85.72805, e \\ ROR_{beta_1}=exp(85.72805) = 1.7 * 10^{35}\\]\n\\[\\beta_2 = -0.05412, e \\ ROR_{beta_2}=exp(-0.05412) = 0.9472426\\]\nInterpretando os betas calculados para cada variável expliativa, temos que\n\nFoi calculado um Regression Odds Ratio de \\(1.7 * 10^{35}\\) para a variável anomalia celular, que possui metricas como área celular, concavidade e textura. Isso indica que quanto maior for o grau de anomalia celular do individuo, maior será a chance de se possuir um tumor maligno. Via ROR, para cada 1 unidade aumentada nessa variável, a probabilidade do individuo possuir um tumor maligno é \\(1.7 * 10^{35}\\) da probabilidade de um individuo com uma unidade a menos\nFoi calculado um Regression Odds Ratio de \\(0.9472426\\) para a variável estrutura celular, que possui metricas como dimensão fractal, suavidade e simetria celular. Isso indica que quanto maior for o grau qualidade da estrutura celular, menor será a chance do indivíduo possuir um tumor maligno. Via ROR, para cada 1 unidade aumentada nessa variável, a probabilidade do individuo possuir um tumor maligno é \\(0.9472426\\) da probabilidade de um individuo com uma unidade a menos\n\nAssim, os betas estimados são bem explicados pelo contexto do problema. Quanto maior o grau de anomalia celular, maior a chance de se possuir um tumor maligno, enquanto, quanto maior for a qualidade da estrutura celular, menor a chance de se possuir um tumor maligno\nO modelo teve o seguinte desempenho no conjunto de dados de treino\n\nMatriz de confusão\n\n\nlibrary(probably)\ncancer_preds =\n  augment(lr_fit, data_cancer_test) |>\n  dplyr::select(diagnosis, contains(\"pred\")) |>\n  mutate(\n      pred = make_two_class_pred(\n      estimate = .pred_B, \n      levels = levels(diagnosis), \n      threshold = .65\n    )\n  ) \n\ncancer_preds |>\n  conf_mat(diagnosis, pred) |>\n  autoplot(type = 'heatmap') \n\n\n\n\nCom as seguintes métricas calculadas\n\ndata.frame(sens = yardstick::sens(cancer_preds,\n                       diagnosis,\n                       pred),\n          spec = yardstick::spec(cancer_preds,\n                       diagnosis,\n                       pred),\n          j_index = yardstick::j_index(cancer_preds,\n                       diagnosis,\n                       pred)\n           ) |>\n  dplyr::select(contains(\"estimate\"))|>\n  kableExtra::kbl() %>%\n  kableExtra::kable_material(c(\"striped\", \"hover\"))\n\n\n\n \n  \n    sens..estimate \n    spec..estimate \n    j_index..estimate \n  \n \n\n  \n    0.9166667 \n    0.8837209 \n    0.8003876 \n  \n\n\n\n\n\nPortanto, a partir da matriz de confusão e das metricas calculadas, vemos um bom desempenho do modelo. Apresentando um baixo erro de falso negativo (alta sensibilidade) e uma boa especificidade. Os valores calculados nos dados de teste foram semelhantes a aqueles calculados nos dados de treino, indicando uma boa generalização do modelo."
  },
  {
    "objectID": "conclusao.html",
    "href": "conclusao.html",
    "title": "Conclusão",
    "section": "",
    "text": "Neste estudo de análise e modelagem de dados de câncer de mama, diversas técnicas foram aplicadas, incluindo análises de redução de dimensionalidade como PCA e FA, engenharia de características, modelos de discriminantes lineares e quadráticos, regressão logística e Random Forest. Após a avaliação dos modelos, constatou-se que o de regressão logística obteve o melhor desempenho, demonstrando alta sensibilidade na detecção de casos positivos de câncer de mama. Esses resultados destacam a importância dessas técnicas na identificação e classificação precisa de casos de câncer de mama, oferecendo perspectivas promissoras para futuras pesquisas e aplicações clínicas."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referências",
    "section": "",
    "text": "Arafa, Ahmed, Marwa Radad, Mohammed Badawy, and Nawal El-Fishawy. 2021.\n“Regularized Logistic Regression Model for Cancer\nClassification.” In 2021 38th National Radio Science\nConference (NRSC), 1:251–61. IEEE.\n\n\nKarhunen, Juha, and Jyrki Joutsensalo. 1995. “Generalizations of\nPrincipal Component Analysis, Optimization Problems, and Neural\nNetworks.” Neural Networks 8 (4): 549–62.\n\n\nNargesian, Fatemeh, Horst Samulowitz, Udayan Khurana, Elias B Khalil,\nand Deepak S Turaga. 2017. “Learning Feature Engineering for\nClassification.” In Ijcai, 17:2529–35.\n\n\nVan Der Maaten, Laurens, Eric O Postma, H Jaap van den Herik, et al.\n2009. “Dimensionality Reduction: A Comparative Review.”\nJournal of Machine Learning Research 10 (66-71): 13."
  }
]